#!/usr/bin/env python3
"""
COMPLETE SIGN LANGUAGE TRAINING & RECOGNITION GUIDE

Step-by-step workflow:
"""

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          SIGN LANGUAGE RECOGNITION - COMPLETE WORKFLOW                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ THREE SIMPLE STEPS:

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

STEP 1: COLLECT TRAINING DATA
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Command:
    python collect_data.py

  What happens:
    â€¢ Shows you each sign one at a time
    â€¢ Displays YOUR hand landmarks in real-time (green skeleton)
    â€¢ You perform the sign
    â€¢ Press SPACE to record 45 frames
    â€¢ Press ESC to skip a sign
    â€¢ Repeats until 50 samples per sign collected

  How to customize WHICH SIGNS:
    Edit collect_data.py, line ~127:
    
      SIGNS = [
          "hello",
          "thank_you",
          "yes",
          "no",
          ...
      ]
    
    Just edit the list to add/remove signs!

  Tips:
    â€¢ Good lighting helps hand detection
    â€¢ Move your hands clearly but not too fast
    â€¢ Different people, angles, speeds = better model

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

STEP 2: TRAIN THE MODEL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Command:
    python train.py

  What happens:
    â€¢ Loads all your collected .npy files
    â€¢ Splits: 80% training, 20% testing
    â€¢ Trains a bidirectional GRU neural network
    â€¢ Shows progress each epoch:
      - Loss: how well model fits data
      - Accuracy: % correct predictions
      - âœ“ Best! = better model found, saves it
    â€¢ Takes ~5-10 minutes on GPU/MPS

  How it works:
    1. Your hand landmark sequences go into the model
    2. Model learns patterns specific to each sign
    3. Tests on unseen data (20%)
    4. Keeps the best version (lowest test loss)

  Customize training:
    Edit train.py, top of file:
      BATCH_SIZE = 32    (higher = faster, needs more RAM)
      EPOCHS = 50        (more = longer training, better model)
      LEARNING_RATE = 0.001  (lower = more stable)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

STEP 3: RUN REAL-TIME RECOGNITION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  Command:
    python -m realtime.realtime_inference

  Or use:
    python run.py

  What happens:
    â€¢ Activates your webcam
    â€¢ Shows live hand landmarks
    â€¢ Every 45 frames, predicts the sign
    â€¢ Uses text-to-speech to say the sign name
    â€¢ Press ESC to quit

  How it works:
    1. MediaPipe detects your hands (green skeleton)
    2. Extracts hand features (positions, distances, angles, motion)
    3. Buffers 45 frames
    4. Feeds to trained model
    5. Model predicts which sign you're making
    6. If confident enough, speaks it out loud

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ FOLDER STRUCTURE (AUTO-CREATED):

  dataset/
    â”œâ”€â”€ hello/
    â”‚   â”œâ”€â”€ 0.npy (50 samples)
    â”‚   â”œâ”€â”€ 1.npy
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ thank_you/
    â”œâ”€â”€ yes/
    â””â”€â”€ ... (one folder per sign)

  models/
    â””â”€â”€ sign_model.pth (your trained model)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”§ TROUBLESHOOTING:

  "No data found" when training?
    â†’ Run collect_data.py first!

  Hand landmarks not showing?
    â†’ Check lighting
    â†’ Make sure hand is visible in frame
    â†’ Move closer to camera

  Poor recognition accuracy?
    â†’ Collect more samples (increase SAMPLES_PER_SIGN)
    â†’ Train for more epochs
    â†’ Be more consistent with your signs

  Model not saving?
    â†’ Check models/ folder exists and is writable

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ TIPS FOR BEST RESULTS:

  1. Consistent signing
     â†’ Use same hand position for each sign
     â†’ Same speed each time
     â†’ Clear hand shapes

  2. Vary conditions
     â†’ Different lighting
     â†’ Different distances from camera
     â†’ Different hand positions
     â†’ This makes model more robust!

  3. More data = better model
     â†’ 50 samples is minimum
     â†’ 100+ samples much better
     â†’ Different people = even better

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Ready? Start with:
  python collect_data.py

""")
